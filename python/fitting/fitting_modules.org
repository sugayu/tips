#+title: *Fittingに使われるPythonモジュールの比較*
#+AUTHOR: sugayu
#+LATEX_CLASS: jsarticle2
#+OPTIONS: toc:nil

* 導入
最尤法(Maximum Likelihood Methods)を提供するデータのフィッティングに使用可能なPythonモジュールは数多くある。
これらを比較することで、
(1) 使用感の違いを見る、
(2) 使い方をメモする、
(3) 最尤値や誤差が正しく求まるか調べる
ことを目的とする。
ベイズ統計やMCMCのパッケージの比較は目的としていない。

* 比較対象のPythonモジュール
** NumPy
~NumPy~ は ~polynomial~ という多項式を扱うモジュールを提供している。
この中で ~fit()~ というメソッドが提供されており、これを使ってモデルの最適化が可能である。
内部で ~numpy.linalg.lstsq()~ という最適化関数を使っており、\( |y - Ax | \) を最小化するという説明があるものの最適化手法は不明。
物理学でよく使われる名前のついた多くの多項式を扱えるが、ガウス関数などは無いので輝線のフィッティングには使えない(と思う)。
~numpy.linalg.lstsq()~ 自体を使えばガウス関数フィッティングもできないこともなさそうではあるが...
- [[https://numpy.org/doc/stable/reference/routines.polynomials-package.html#module-numpy.polynomial][numpy.polynomial — NumPy v2.2 Manual]]
- [[https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq][numpy.linalg.lstsq — NumPy v2.2 Manual]]

** Scipy
~SciPy~ は ~optimize~ という最適化のためのモジュールを提供している。
この中で ~curve_fit()~ という関数は非線形最小二乗法によりモデルの最適化を行う。
~method~ 引数により最適化手法をLevenberg-Marquardt法、Trust Region Reflective法、dogleg法の中から選べる。
デフォルトではパラメータ範囲に制限が無ければLM法、あればTRR法が使われる。
LM法の場合は最尤推定にFortranで実装されたMINPACKというプログラムを呼び出す ~scipy.optimize.leastsq()~ を使用しており、
他の手法の場合は最尤推定に ~scipy.optimize.least_square()~ という新しい関数を使用している。
データの誤差 ~sigma~ の中に誤差の相関(共分散)を入れることもできる。
- [[https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html][curve_fit — SciPy v1.15.2 Manual]]

2変数の間の線形回帰は ~linregress()~ という関数でも与えられており、こちらを使えば関数を指定してやる必要がない。
ただしデータ点に誤差を与えることができない。
内部では ~np.cov()~ を使って共分散を計算し、解析解を導いているようである。
(なお ~linregress~ はlinear regressionの略だと思われる。)
- [[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html#scipy.stats.linregress][linregress — SciPy v1.15.2 Manual]]

** Astropy
天文解析パッケージ ~Astropy~ は ~modeling~ というモジュールを提供している。
天文関係で使うと思われるモデルやフィッティングの関数、単位付き変数のフィッティングが整備されているので、
使いたいモデルが実装されていないか確認する価値がある。
- [[https://docs.astropy.org/en/stable/modeling/][Models and Fitting (astropy.modeling) — Astropy v7.0.1]]
- [[https://docs.astropy.org/en/stable/modeling/models.html][Models — Astropy v7.0.1]]

~modeling.models~ モジュールが提供するクラスを使って統計モデルを構築し、
~modeling.fitting~ モジュールが提供するクラスを使ってモデルをデータに当て嵌める。
しかし、結局中身は ~scipy.optimize.least_squares()~ を使っている。
User Interfaceが関数ではなくクラス中心で複雑なので、それに応じた複雑なフィッティングをするのに適していると考えられる。
内部で変数変換したり ~scipy.optimize~ のフィッティング関数のアウトプットを変換したりしているので、
コードを見るとフィッティング結果をどう扱うかの勉強になるかも？

用意されているおすすめの最適化手法は、単純な線形の場合は ~LinearLSQFitter~、
非線形の場合は ~TRFLSQFitter~, ~DogBoxLSQFitter~, ~LMLSQFitter~ の3種類である。
これらの最適化手法は内部で ~scipy.optimize~ を使っているが、~LinearLSQFitter~ だけは例外的に ~numpy.linalg.lstsq()~ を使っている。
パラメータ範囲が指定されていない小さな最適化であれば ~LMLSQFitter~,
範囲が指定されているなら ~TRFLSQFitter~, ~DogBoxLSQFitter~ が良いとのこと。
アルゴリズムが不安定なので、 ~LMLSQFitter~ を使う場合はパラメータ範囲を指定できない。
- [[https://docs.astropy.org/en/stable/modeling/fitting.html][Fitting Models to Data — Astropy v7.0.1]]

** specutils
天体スペクトルを扱うパッケージ ~specutils~ は輝線フィッティングを含めたスペクトル解析のためのモジュールを提供する。
フィッティングの際には内部で ~Astropy.modeling~ モジュールが呼ばれている。
ユーザーの輝線フィッティングを助けるパッケージとも言える。
- [[https://specutils.readthedocs.io/en/stable/fitting.html][Line/Spectrum Fitting — specutils v1.19.1.dev0+g746a5d4.d20241105]]

** MPFIT
~MPFIT~ は古くから天文学で使われているフィッティングパッケージ。
元々はFortranで書かれた ~MINPACK-1~ というパッケージの中のフィッティングコードをCraig MarkwardtさんがIDLで書き直した。
それをMark RiversさんがPythonで書き直し、Sergey Koposovさんが ~numpy~ で実装し直した後にPython3で動くようにした。
IDLでフィッティングといえばこのコード。
Levenberg-Marquardt法を使っているので効率良く最小二乗問題を解けるのが売りだった。
- [[https://eispac.readthedocs.io/en/stable/guide/07-mpfit_docs.html][MPFIT Documentation — eispac 0.1.dev108+gdfa97b1 documentation]]
- [[https://github.com/segasai/astrolibpy/blob/master/mpfit/mpfit.py][astrolibpy/mpfit/mpfit.py at master · segasai/astrolibpy · GitHub]]

~pip~ でダウンロードできる形式では配布されていない。
~astrolibpy~ をダウンロードしたあと、Pythonのパスが通っている場所に ~mpfit~ のディレクトリを置いたら良い。
たぶんディレクトリ直下に mpfit/__init__.py ファイルを設置する必要がある (空ファイルで良い)。

** LMFIT
~LMFIT~ は非線形最小二乗法を解くためのパッケージ。
~scipy.optimize~ モジュールから着想を得て、フィッティングのための便利な機能を多数導入している。
多くのモデル、多様な最適化手法、パラメータ制御法、解の解析手法が提供されている。
- [[https://lmfit.github.io/lmfit-py/index.html][Non-Linear Least-Squares Minimization and Curve-Fitting for Python — Non-Linear Least-Squares Minimization and Curve-Fitting for Python]]

基本的に最適化には ~scipy.optimize~ が内部で使われており、 例えば ~method='leastsq'~ が指定されているときは ~scipy.optimize.leastsq()~ が使用されている。
~Minimizer.minimize()~ の説明:
#+begin_example
  In most cases, these methods wrap and use the method with the same name from `scipy.optimize`, or use `scipy.optimize.minimize` with the same `method` argument.
#+end_example

[[https://lmfit.github.io/lmfit-py/intro.html][Getting started]]で述べられている ~scipy.optimize.leastsq()~ から改善したかった点は、
1. パラメータを(本質的に意味のない)数値インデックスで指定する(~p[0]~)のではなく、名前で指定したい(~p['slope']~)。
2. パラメータを固定したかったら作った関数を変更しなければならない。
3. パラメータ範囲を指定する方法が頑強な方法が無いし、あったとしても全パラメータの範囲を順番に指定しなければならない。
4. パラメータの振舞いを制限する方法がかなり複雑 (~p['b'] = 3*p['a']~ としたいときなど。)。

また ~emcee~ を使って最尤推定値まわりの事後分布を得る関数も提供されている。
これはパラメータの確率分布(誤差)を調べるためであって、パラメータ推定のために用意されているわけではないと何度も注意されている。
- [[https://lmfit.github.io/lmfit-py/fitting.html#minimizer-emcee-calculating-the-posterior-probability-distribution-of-parameters][Performing Fits and Analyzing Outputs — Non-Linear Least-Squares Minimization and Curve-Fitting for Python]]

* 理想的な線形データのフィッティング結果
** データ作成
- 平均: 100.0
- 標準偏差: 10.0
- 標本の大きさ: 30
- 直線: \(y = 2.0 (x - 100.0) + 220.0\)
#+begin_src ipython :session :ipyfile ./obipy-resources/data.png :exports both :async t :results raw drawer :eval never-export
  import numpy as np
  from numpy.random import default_rng
  from sugayutils.figure import makefig

  rng = default_rng(222)

  size = 30
  sigma = np.full(size, 10.0)
  noise = rng.standard_normal(size) * sigma
  x0 = 100.0
  x = rng.normal(x0, 10, size=size)
  xn = x - x0

  a, b = 2.0, 220.0
  y0 = a * (x - x0) + b
  y = y0 + noise

  fig = makefig(figsize=['small', 1.0])
  ax = fig.add_subplot(1, 1, 1)
  _ = ax.scatter(x, y)
#+end_src

#+RESULTS:
:results:
# Out[2]:
[[file:./obipy-resources/data.png]]
:end:

** 解析解
一次方程式の場合は解析解が得られている。
係数の最尤推定値は
\begin{align}
\label{eq:1}
  a &= \frac{N\sum x_i y_i - \sum x_i \sum y_i}{N\sum x_i^2 - (\sum x_i)^2} \\
  b &= \frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i}{N\sum x_i^2 - (\sum x_i)^2}
\end{align}
であり、その誤差は
\begin{align}
\label{eq:2}
  \sigma_\text{a} & = \sigma \sqrt{\frac{N}{N\sum x_i^2 - (\sum x_i)^2}} \\
  \sigma_\text{b} & = \sigma \sqrt{\frac{\sum x_i^2}{N\sum x_i^2 - (\sum x_i)^2}} \\
\end{align}
と表せる。
- [[http://www.cc.u-ryukyu.ac.jp/~fukami/p0.pdf][物理実験III データ処理 (琉球大学深水研究室)]]

以上より最尤推定値を求める。
#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  denom = size * np.sum(xn**2) - np.sum(xn) ** 2
  sol_analytic = {
      'a': (size * np.sum(xn * y) - np.sum(xn) * np.sum(y)) / denom,
      'b': (np.sum(xn**2) * np.sum(y) - np.sum(xn) * np.sum(xn * y)) / denom,
      's_a': sigma[0] * np.sqrt(size / denom),
      's_b': sigma[0] * np.sqrt(np.sum(xn**2) / denom),
  }
  sol_analytic
#+end_src

#+RESULTS:
:results:
# Out[3]:
#+BEGIN_EXAMPLE
  {'a': 1.8419873744634017,
  'b': 221.09327400439,
  's_a': 0.18305375486749972,
  's_b': 1.8375169284378194}
#+END_EXAMPLE
:end:

1sigma誤差の範囲に真値が収まっている。

** Numpy
~Polynomial.fit()~ を使ったフィッティング手法を示す。
デフォルトの返り値は ~Polynomial~ インスタンスである。
~full=True~ のキーワード引数を与えるとタプルを出力し、2番目の要素にフィッティングの情報が含まれる。
- [[https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit][numpy.polynomial.polynomial.Polynomial.fit — NumPy v2.2 Manual]]
#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from numpy.polynomial import Polynomial

  p, status = Polynomial.fit(xn, y, 1, w=1 / sigma, full=True)
  p = p.convert()

  sol_numpy = {'a': p.coef[1], 'b': p.coef[0], 's_a': 0.0, 's_b': 0.0}
  sol_numpy
#+end_src

#+RESULTS:
:results:
# Out[4]:
: {'a': 1.8419873744634028, 'b': 221.09327400438997, 's_a': 0.0, 's_b': 0.0}
:end:

最尤推定値は解析解とほとんど正確に一致したが、誤差を出力してくれないようである。
なお、 ~Polynomial.fit()~ は ~Polynomial~ クラスのクラスメソッドである。

** Scipy
*** curve_fit
引数 ~absolute_sigma=True~ にすると誤差 ~sigma~ を絶対値で設定することになる。
デフォルトは ~absolute_sigma=False~ なので相対値で指定、返り値の共分散 ~pcov~ も相対値になるので注意する。
~pcov~ の絶対値と相対値の関係は ~pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)~ 。
引数 ~full_output=True~ でフィッティングに関する細かい出力が得られる。
他にも ~bounds~ や ~loss~ など多くの引数を持つ。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from scipy.optimize import curve_fit


  def func(x, a, b):
      return a * x + b


  popt, pcov, infodict, mesg, ier = curve_fit(
      func, xn, y, sigma=sigma, absolute_sigma=True, full_output=True
  )
  perr = np.sqrt(np.diag(pcov))

  sol_scipy_curvefit = {'a': popt[0], 'b': popt[1], 's_a': perr[0], 's_b': perr[1]}
  sol_scipy_curvefit
#+end_src

#+RESULTS:
:results:
# Out[5]:
#+BEGIN_EXAMPLE
  {'a': 1.84198738877173,
  'b': 221.09327399514368,
  's_a': 0.18305375794763726,
  's_b': 1.8375169220175631}
#+END_EXAMPLE
:end:

~numpy.Polynomial.fit~ と同じ結果を示したが、解析解からの数値誤差が若干ある(実用的に何の問題もないし気にするべきではない)。
結果には示していないが、pcovの値を見ると共分散項はおよそ-0.04であり、
最適化されたパラメータ間の相関(共分散)がほとんどゼロであることが分かる。

*** linregress
引数 ~alternative~ を加えることで検定も可能らしい。
誤差 ~sigma~ を与えることはできない。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from scipy import stats

  res = stats.linregress(xn, y)
  sol_scipy_linregress = {
      'a': res.slope,
      'b': res.intercept,
      's_a': res.stderr,
      's_b': res.intercept_stderr,
  }
  sol_scipy_linregress
#+end_src

#+RESULTS:
:results:
# Out[7]:
#+BEGIN_EXAMPLE
  {'a': 1.8419873744634003,
  'b': 221.09327400439,
  's_a': 0.22700227671955325,
  's_b': 2.2786777936788623}
#+END_EXAMPLE
:end:

最尤推定値は解析解と一致した。
誤差が解析解より少し大きいのは、おそらく内部でデータの誤差を相関係数やデータの標準偏差(標本標準偏差)から評価しているから。
実際、 ~np.std(noise)~ の値は12であり、最尤推定値の誤差も同じくらいの倍率で大きくなっている(完全に一致はしない)。

** Astropy
*** LinearLSQFitter
LinearLSQFitterの場合。内部で ~numpy.linalg.lstsq()~ を使っているので誤差の出力は無し。
#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from astropy.modeling import models, fitting

  fit = fitting.LinearLSQFitter()
  line_init = models.Linear1D()   # initial values <Linear1D(slope=1., intercept=0.)>
  fitted_line = fit(line_init, xn, y, weights=1 / sigma)
  sol_astropy_linear = {'a': fitted_line.slope.value, 'b': fitted_line.intercept.value, 's_a': 0.0, 's_b': 0.0}
  sol_astropy_linear
#+end_src

#+RESULTS:
:results:
# Out[8]:
: {'a': 1.8419873744634014, 'b': 221.09327400439003, 's_a': 0.0, 's_b': 0.0}
:end:

内部の実装どおり ~numpy.Polynomial.fit()~ と同じ結果が得られ、解析解と一致した。

*** LMLSQFitter
~LMLSQFitter~ は内部で ~scipy.optimize.least_squares()~ を使っている。
引数 ~calc_uncertainties=True~ を与えるとパラメータ誤差を計算して ~fitted_line.cov_matrix~ と ~fitted_line.stds~ に値が入力される。
この引数を与えなくても、 ~fit['fit_info']~ にフィッティングの結果は残されている。
ちなみに、 ~scipy.optimize.curve_fit()~ のデフォルトの結果を修正して、与えた誤差は絶対値 (~absolute_sigma=True~) になるように内部で補正されている。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from astropy.modeling import models, fitting

  fit = fitting.LMLSQFitter(calc_uncertainties=True)
  line_init = models.Linear1D()  # initial values <Linear1D(slope=1., intercept=0.)>
  fitted_line = fit(line_init, xn, y, weights=1 / sigma)
  sol_astropy_LM = {
      'a': fitted_line.slope.value,
      'b': fitted_line.intercept.value,
      's_a': fitted_line.stds['slope'],
      's_b': fitted_line.stds['intercept'],
  }
  sol_astropy_LM
#+end_src

#+RESULTS:
:results:
# Out[9]:
#+BEGIN_EXAMPLE
  {'a': 1.8419873744634017,
  'b': 221.09327400438997,
  's_a': 0.1830537548674997,
  's_b': 1.8375169284378194}
#+END_EXAMPLE
:end:

なぜか ~scipy.optimize.curve_fit()~ よりも ~LinearSQFitter~ に近い結果が得られた。
~curve_fit()~ は内部で ~scipy.optimize.leastsq()~ を使用しており、 ~LMLSQFitter~ は ~scipy.optimize.least_square()~ を使用しているので、
内部のわずかな実装の違いが表れたのかもしれない。
もちろん、誤差の範囲ではこれらは一致している。
得られた誤差も ~scipy.optimize.curve_fit()~ に近い値が得られた。

なお、これを実行すると
#+begin_example
  WARNING: Model is linear in parameters; consider using linear fitting methods. [astropy.modeling.fitting]
#+end_example
という警告が出る。
線形フィッティングは ~LinearLSQFitter~ がお薦めのようである (誤差を出力してくれないのに？)。

** MPFIT
コードの表記がPythonを使ううえであまり直感的ではないので、使うならさらにwrapperを作ってしまうのが便利な気がする。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  from mpfit.mpfit import mpfit
  import numpy as np


  def func(p, fjac=None, x=None, y=None, err=None):
      y0 = p[0] * x + p[1]
      status = 0
      return [status, (y - y0) / err]


  p0 = [1.0, 200.0]
  functkw = {'x': xn, 'y': y, 'err': sigma}
  m = mpfit(func, p0, functkw=functkw)

  sol_mpfit = {
      'a': m.params[0],
      'b': m.params[1],
      's_a': m.perror[0],
      's_b': m.perror[1],
  }
  sol_mpfit
#+end_src

#+RESULTS:
:results:
# Out[11]:
#+BEGIN_EXAMPLE
  {'a': 1.8419873769467103,
  'b': 221.09327423192875,
  's_a': 0.18305375489620923,
  's_b': 1.8375169218509713}
#+END_EXAMPLE
:end:

同じMINPACK由来のコードと言えど他の手法とは実装が異なるので、異なる数値誤差が出た。

デフォルトではプリントメッセージが表示される。
#+begin_example
  Iter       1    CHI-SQUARE =  210.0576791  DOF =  28
     P0 = 1  
     P1 = 200  
  Iter       2    CHI-SQUARE =  43.05872579  DOF =  28
     P0 = 1.841987377  
     P1 = 221.0932742  
#+end_example
まじか、Levenberg-Marquardt法ってIteration 1回で終わるのか... 信じられへんな...

** LMFIT
*** 関数を使った書き方
書き方は ~MPFIT~ と似ているところがある。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  import numpy as np
  import lmfit


  def func(p, x, data, uncertainty):
      y0 = p['slope'] * x + p['intercept']
      return (data - y0) / uncertainty


  params = lmfit.create_params(slope=1.0, intercept=200.0)
  out = lmfit.minimize(func, params, args=(xn, y, sigma))

  sol_lmfit = {
      'a': float(out.params['slope'].value),
      'b': float(out.params['intercept'].value),
      's_a': float(out.params['slope'].stderr),
      's_b': float(out.params['intercept'].stderr),
  }
  sol_lmfit
#+end_src

#+RESULTS:
:results:
# Out[15]:
#+BEGIN_EXAMPLE
  {'a': 1.8419873744056405,
  'b': 221.09327400473325,
  's_a': 0.22700227671667123,
  's_b': 2.2786777936860263}
#+END_EXAMPLE
:end:

~scipy.optimize.leastsq()~ を使っているが、 ~scipy.optimize.curve_fit()~ と数値誤差が完全に同じというわけではなさそう。
誤差が大きくなって ~scipy.stats.linregress~ と同じになっているが、これは *内部で勝手にデータから誤差をスケールさせている* から。
~scale_covar=False~ を指定すると他の手法と同じ結果が得られる。
スケールすること自体は悪くなはないが、自分の使っている手法が中で何をやっているかはちゃんと理解しておく必要があるだろう。

*** Modelクラスを使った書き方
同じ ~lmfit~ でも ~Model~ クラスを使った書き方もある。
既存のフィッティング関数を使ったり、それと自分のオリジナルの関数を組み合わせたりする場合はこちらが便利かもしれない。

#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  import numpy as np
  from lmfit.models import LinearModel


  model = LinearModel()
  init = model.guess(y, x=xn)
  out = model.fit(y, init, x=xn, weights=1 / sigma)

  sol_lmfit2 = {
      'a': float(out.params['slope'].value),
      'b': float(out.params['intercept'].value),
      's_a': float(out.params['slope'].stderr),
      's_b': float(out.params['intercept'].stderr),
  }
  sol_lmfit2
#+end_src

#+RESULTS:
:results:
# Out[17]:
#+BEGIN_EXAMPLE
  {'a': 1.84198737447337,
  'b': 221.09327400437255,
  's_a': 0.22700227671776427,
  's_b': 2.278677793677153}
#+END_EXAMPLE
:end:

数値誤差も含めると上の手法と完全に一致するわけではない。なぜ？
こちらも ~Model.fit()~ に ~scale_covar~ という引数がある。

** まとめ
#+begin_src ipython :session :exports both :async t :results raw drawer :eval never-export
  import pandas as pd

  data = {
      'Analytic': sol_analytic,
      'Numpy': sol_numpy,
      'Scipy curvefit': sol_scipy_curvefit,
      'Scipy linregress': sol_scipy_linregress,
      'Astropy Linear': sol_astropy_linear,
      'Astropy LM': sol_astropy_LM,
      'MPFIT': sol_mpfit,
      'LMFIT minimize': sol_lmfit,
      'LMFIT Model.fit': sol_lmfit2,
  }
  pd.DataFrame.from_dict(data, orient='index')
#+end_src

#+RESULTS:
:results:
# Out[29]:
#+BEGIN_EXAMPLE
                           a           b       s_a       s_b
  Analytic          1.841987  221.093274  0.183054  1.837517
  Numpy             1.841987  221.093274  0.000000  0.000000
  Scipy curvefit    1.841987  221.093274  0.183054  1.837517
  Scipy linregress  1.841987  221.093274  0.227002  2.278678
  Astropy Linear    1.841987  221.093274  0.000000  0.000000
  Astropy LM        1.841987  221.093274  0.183054  1.837517
  MPFIT             1.841987  221.093274  0.183054  1.837517
  LMFIT minimize    1.841987  221.093274  0.227002  2.278678
  LMFIT Model.fit   1.841987  221.093274  0.227002  2.278678
#+END_EXAMPLE
:end:

(誤差のスケールを除いて) 全ての数値が一致した。

** 図
~LMFIT~ は手軽にベストフィット関数の誤差範囲を示すことができる。
- [[https://lmfit.github.io/lmfit-py/examples/documentation/model_uncertainty_pred.html#sphx-glr-examples-documentation-model-uncertainty-pred-py][Model - uncertainty pred — Non-Linear Least-Squares Minimization and Curve-Fitting for Python]]
計算は以下のサイトに基づいているよう。
- [[https://www.astro.rug.nl/software/kapteyn/kmpfittutorial.html#confidence-and-prediction-intervals][Least squares fitting with kmpfit — Kapteyn Package (home)]]
  (新しい ~kmpfit~ が登場したが、さすがにもうやってられない)

~scale_covar=False~ を与えて ~lmfit~ で計算し直した。
また、 ~xx - x0~ をどこで使うか少し考えないといけないことに注意。

#+begin_src ipython :session :ipyfile ./obipy-resources/fit_linear.png :exports both :async t :results raw drawer :eval never-export
  from sugayutils.core import colors as col
  from sugayutils.figure import makefig

  out = model.fit(y, init, x=xn, weights=1 / sigma, scale_covar=False)

  xx = np.linspace(80, 130, 101)
  yy = out.eval(x=xx - x0)
  ye = out.eval_uncertainty(x=xx - x0, sigma=1)

  fig = makefig(figsize=['small', 1.0])
  ax = fig.add_subplot(1, 1, 1)
  ax.scatter(x, y, c='blue', zorder=2)
  ax.plot(xx, yy, c='red', zorder=1)
  ax.plot(xx, a * (xx - x0) + b, c='blue', ls='--', zorder=1)
  ax.fill_between(xx, yy - ye, yy + ye, color=col.bpink, zorder=0)
#+end_src

#+RESULTS:
:results:
# Out[44]:
: <matplotlib.collections.FillBetweenPolyCollection at 0x7fc99e5dfd40>
[[file:./obipy-resources/fit_linear.png]]
:end:

データを生成した新の直線を青破線、最尤推定した直線を赤実線、その誤差を淡い赤帯で示した。

* 理想的な輝線データのフィッティング結果
** データ作成
** Numpy
** Scipy
** Astropy
** specutils
** MPFIT
** LMFIT
** まとめ
